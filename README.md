# SINP
Structure and interpretation of neural programs.

An interactive, notebook-native textbook that teaches neural networks and large-language models from first principles‚Äîmodeled after Structure and Interpretation of Computer Programs (SICP) but written in modern Python.

‚ÄúPrograms must be written for people to read, and only incidentally for machines to execute.‚Äù ‚Äî Abelson & Sussman  (adapted for gradients by SINP)

‚∏ª

üìö What You‚Äôll Find Here

Folder	Purpose
part01_building_abstractions/	Foundational math, tensors, & automatic differentiation
part02_state_and_modularity/	Parameter objects, optimizers, data streams, a micro-framework
part03_transformer_interpreter/	Attention, tokenization, encoder/decoder models
part04_scaling_up/	Mixed precision, distributed training, scaling laws, RAG
part05_reflection_and_ethics/	Probing, alignment, serving, open problems
assets/	SVG diagrams & printable figures
environments/	Conda & Dev-container specs for reproducible installs
tests/	pytest + nbval suites that keep notebooks executable
examples/	Finished capstone projects contributed by the community

Each chapter lives in its own numbered notebook, ending with executable exercises and starred research prompts (‚òÖ).

‚∏ª

üöÄ Quick Start (5 minutes)

# 1 Clone the repo
$ git clone https://github.com/<your_org>/sinp.git && cd sinp

# 2 Create the environment (Conda)
$ conda env create -f environments/environment.yml
$ conda activate sinp

# 3 Launch JupyterLab
$ jupyter lab

Prefer VS Code Dev Containers?

$ docker compose up  # or ‚ÄúDev Containers: Open Folder in Container‚Äù in VS Code

Note : The default environment targets Python ‚â• 3.11 with CUDA 11.8.   CPU-only? Add DEVICE=cpu before running docker compose.

‚∏ª

üìñ Book Outline

Preface
	‚Ä¢	Why another NN/LLM book?
	‚Ä¢	How to use this text & the Jupyter/Colab notebook set
	‚Ä¢	Suggested pacing for self-study, university semesters, and intensive boot-camps
	‚Ä¢	Conventions & a quick tour of Python ‚â• 3.11, pytest, and ruff for literate tests

‚∏ª

Part I ‚Äî Building Abstractions with Functions & Data

Ch	Title	Core Ideas	Notebook Lab
1	A Pythonic Calculus of Procedures	Pure vs. stateful functions, higher-order funcs, closures	01_hof_basics.ipynb ‚Äì build compose, curry, simple REPL
2	Numbers with Errors	Rational, complex, interval & automatic differentiation (dual numbers)	02_interval_dual.ipynb ‚Äì exact rational class; forward AD for sin/cos
3	Vectors ‚Üí Tensors as Abstract Data	Tensor shape algebra, broadcasting, memory layout	03_tensor_algebra.ipynb ‚Äì minimal NumPy-free tensor class
4	The Meta-Differentiable Evaluator	Reverse-mode AD via an interpreter that records elementary ops	04_autograd_from_scratch.ipynb ‚Äì tiny Autograd clone

SICP Resonance: These chapters mirror SICP‚Äôs generic arithmetic & meta-circular evaluator, but with differentiation as the unifying abstraction.

‚∏ª

Part II ‚Äî Modularity, Objects, & State in Learning Systems

Ch	Title	Core Ideas	Notebook Lab
5	Parameter Objects & Mutation	Why learning needs mutable state; SGD as controlled side-effect	05_sgd_optimizer.ipynb ‚Äì write dense layer & vanilla SGD
6	Sequences, Recursion, & Scan Ops	RNNs via higher-order scan; timestep unrolling vs. symbolic gradients	06_rnn_scan.ipynb ‚Äì build a char-level RNN from scratch
7	Mini-Framework: tinygrad-SICP	Assemble modules into a micro-framework (<200 LOC)	07_framework.ipynb ‚Äì CIFAR-10 training in under 1 CPU-hour
8	Streams, Generators, & Data Pipes	Lazy evaluation for infinite datasets, TFRecord/Parquet ingestion	08_streams.ipynb ‚Äì build sharded token stream with Python generators


‚∏ª

Part III ‚Äî Metalinguistic Abstraction: The Transformer as an Interpreter

Ch	Title	Core Ideas	Notebook Lab
9	From Attention to Self-Interpretation	Dot-product attention as a dispatch table	09_attention.ipynb ‚Äì visualize key/query overlap heatmaps
10	Building a Transformer Encoder	Multi-head, layer norm, residual pathways	10_transformer_encoder.ipynb ‚Äì train tiny BERT on toy corpus
11	Byte-Pair Encoding & Token Semirings	Compression as abstraction; implement BPE, unigram, WordPiece	11_tokenizer.ipynb ‚Äì learn BPE rules from Shakespeare
12	Autoregressive Decoders & Causal Masks	Parallel scan, KV-cache, temperature sampling	12_decoder.ipynb ‚Äì sampler with nucleus & repetition penalties


‚∏ª

Part IV ‚Äî Scaling Up: From Toy Models to LLMs

Ch	Title	Core Ideas	Notebook Lab
13	Optimizers Beyond SGD	Adam, Adafactor, Lion‚Äîderive from first principles	13_optimizer_family.ipynb ‚Äì compare on MNIST-1D
14	Mixed Precision & Hardware Kernels	FP8/FP16 numerics, CUDA kernels, intro to triton	14_fused_kernels.ipynb ‚Äì write a fused softmax in Triton
15	Distributed Data & Model Parallel	Pipeline vs. tensor vs. expert parallel	15_distributed_training.ipynb ‚Äì run DDP on 2-GPU simulation
16	Scaling Laws & Curriculum	Empirical power laws, Chinchilla optimality	16_scaling_laws.ipynb ‚Äì reproduce Kaplan curves with synthetic data
17	In-Context Learning & Function Calling	Prompt engineering, retrieval augmentation	17_icl_tools.ipynb ‚Äì build a simple RAG pipeline


‚∏ª

Part V ‚Äî Reflection, Interpretation, & Ethics

Ch	Title	Core Ideas	Notebook Lab
18	Analyzing Internal Representations	Probing classifiers, causal tracing	18_rep_probing.ipynb ‚Äì detect neuron directions for sentiment
19	Alignment & Value Learning	RLHF, Constitutional AI, interpretability tooling	19_rlhf_minimal.ipynb ‚Äì preference-based RL on toy tasks
20	Systems Engineering for Production	Serving, quantization, streaming, cold-start latency	20_serving.ipynb ‚Äì deploy a quantized model with FastAPI
21	Limits, Open Problems, & Next Steps	Non-IID data, continual learning, neurosymbolic hybrids	21_future.ipynb ‚Äì experiment: hybrid transformer + SAT solver


‚∏ª

Epilogue
	‚Ä¢	The meta-circular gradient: deriving AD inside an LLM
	‚Ä¢	Further reading: DSP, Deep Learning, PLDI transformer papers
	‚Ä¢	Historical timeline from McCulloch & Pitts (1943) to GPT-4o (2024)

‚∏ª

Pedagogical Features (SICP-Inspired)
	1.	Executable Exercises ‚Äì numbered tasks; ‚òÖ problems are open-ended research prompts.
	2.	Orange Boxes ‚Üí ‚ÄúFoot-Guns Ahead‚Äù ‚Äì common implementation pitfalls.
	3.	Purple Boxes ‚Üí ‚ÄúAbstraction Barriers‚Äù ‚Äì highlight intentional boundary crossings.
	4.	Capstone Projects ‚Äì e.g., build a 70 M-parameter GPT-Mini on TinyStories.
	5.	Meta-Circular Threads ‚Äì recurring theme: ‚Äúa learner that can explain its own gradient‚Äù.
	6.	Interleaved Essays ‚Äì ethics, scaling economics, hardware trends.

‚∏ª

üèÉ‚Äç‚ôÄÔ∏è Running a Chapter Notebook
	1.	Open part01_building_abstractions/01_hof_basics.ipynb in JupyterLab.
	2.	Execute cells top-to-bottom (Shift+Enter).
	3.	Complete the Exercises‚Äîanswers are validated by hidden tests.

Hints & solutions live in collapsible Spoilers cells.

‚∏ª

üì¶ Tested Configurations

Platform	Python	GPU	Status
Ubuntu 22.04	3.11 / 3.12	RTX 4090 (CUDA 11.8)	‚úÖ all parts
macOS 14 (Apple Silicon)	3.11	M-series (Metal)	‚úÖ Parts I-III
Windows 11 WSL2	3.11	RTX 30xx	‚úÖ tested

CI runs on GitHub Actions for CPU notebooks; GPU notebooks are smoke-tested nightly.

‚∏ª

ü§ù Contributing
	1.	Fork ‚û° Branch ‚û° PR.
	2.	Run ruff check . --fix and pytest -q; both must pass.
	3.	For new notebooks, add three automated tests in tests/.
	4.	Agree to the lightweight Contributor License Agreement in your PR description.

We love bug reproductions, exercise ideas, and real-world datasets for capstones.

‚∏ª

üìú License

¬© 2025 MIT License‚Äîsee LICENSE for details.  Diagrams under CC-BY 4.0.

Commercial use? Reach out via Issues.

‚∏ª

üîñ Citation

@misc{skryl2025sinp,
  title  = {Structure and Interpretation of Neural Programs},
  author = {Alex Skryl and Contributors},
  year   = {2025},
  url    = {https://github.com/<your_org>/sinp}
}


‚∏ª

üôè Acknowledgements
	‚Ä¢	Gerald Jay Sussman & Hal Abelson for SICP‚Äôs timeless pedagogy.
	‚Ä¢	Inspirations: tinygrad, micrograd, minGPT, nanoGPT, lit-gpt.
	‚Ä¢	Early testers in the #sinp-alpha Discord.

Happy gradient-hacking!
